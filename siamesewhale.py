# -*- coding: utf-8 -*-
"""SiameseWhale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-JMNc_G-sigpRTURFfvFi_HTxuDcbrIb
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import pickle
from math import sqrt
import matplotlib.pyplot as plt
import random
from keras import backend as K
from keras.preprocessing.image import img_to_array,array_to_img
from scipy.ndimage import affine_transform
import keras
from PIL import Image
from keras import applications,optimizers
from keras.models import Model,Sequential,Input
from keras.layers import Dense,Activation,Lambda,Concatenate,Reshape
from keras.utils import Sequence
from tqdm import tqdm_notebook as tqdm
import time 

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
#print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.

from google.colab import drive
drive.mount('/content/gdrive')
!ls

# %cd gdrive/
!ls

# %cd 'My Drive'
!ls

# %cd  Humpback-whale-identification

np.random.seed(42)
TRAIN_CSV='train.csv'
TRAIN_DIR='train/'
TEST_DIR='test/'
BB='bounding_boxes.csv'
P2H='p2h.pickle'
P2S='p2size.pickle'
tagged=dict([(p,w) for a,p,w in pd.read_csv(TRAIN_CSV).to_records()])

bounding_pd=pd.read_csv(BB)   #Bounding Box Coordinates
bounding_pd.head()
boxes_coord=dict( [(p,(x0,y0,x1,y1)) for a,p,x0,y0,x1,y1 in pd.read_csv(BB).to_records()])

h2ws={}
for k,v in tagged.items():
    if v!='new_whale':
        if v not in h2ws: h2ws[v]=[k]
        else: h2ws[v].append(k)
            
print(len(h2ws))   

train=[]
for k,v in h2ws.items():
    if len(v)>1:
        train+=v
random.shuffle(train)        
print(len(train))        

w2ts={}

for k,v in tagged.items():
    if k in train:
        if v not in w2ts: w2ts[v]=[k]
        else: w2ts[v].append(k)
            
for k,v in w2ts.items():
    w2ts[k]=np.array(v)
    
print(len(w2ts))

position={}

for i,t in enumerate(train):
    position[t]=i
        
pickle_out = open("pos.pickle","wb")
pickle.dump(position, pickle_out)
pickle_out.close()

from os.path import isfile
def expand_path(file):
    if isfile(TRAIN_DIR+file): return TRAIN_DIR+file
    elif isfile(TEST_DIR+file): return TEST_DIR+file

img_shape=(299,299,3)
img_shape2=(299,299,1)
anisotropy=2.15
crop_margin=0.05

def read_raw_image(p):
    img = Image.open(expand_path(p))
    return img

def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):
    """
    Build a transformation matrix with the specified characteristics.
    """
    rotation        = np.deg2rad(rotation)
    shear           = np.deg2rad(shear)
    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])
    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])
    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])
    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])
    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])
    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))

def read_cropped_image(p, augment):
    """
    @param p : the name of the picture to read
    @param augment: True/False if data augmentation should be performed
    @return a numpy array with the transformed image
    """
   
    img   = read_raw_image(p).convert('L')
    img   = img_to_array(img)
    
    size_x,size_y,z = img.shape
    
    # Determine the region of the original image we want to capture based on the bounding box.
    x0,y0,x1,y1   = boxes_coord[p]
    
    dx            = x1 - x0
    dy            = y1 - y0
    x0           -= dx*crop_margin
    x1           += dx*crop_margin + 1
    y0           -= dy*crop_margin
    y1           += dy*crop_margin + 1
    if (x0 < 0     ): x0 = 0
    if (x1 > size_x): x1 = size_x
    if (y0 < 0     ): y0 = 0
    if (y1 > size_y): y1 = size_y
    dx            = x1 - x0
    dy            = y1 - y0
    if dx > dy*anisotropy:
        dy  = 0.5*(dx/anisotropy - dy)
        y0 -= dy
        y1 += dy
    else:
        dx  = 0.5*(dy*anisotropy - dx)
        x0 -= dx
        x1 += dx

    # Generate the transformation matrix
    trans = np.array([[1, 0, -0.5*img_shape[0]], [0, 1, -0.5*img_shape[1]], [0, 0, 1]])
    trans = np.dot(np.array([[(y1 - y0)/img_shape[0], 0, 0], [0, (x1 - x0)/img_shape[1], 0], [0, 0, 1]]), trans)
    if augment:
        trans = np.dot(build_transform(
            random.uniform(-5, 5),
            random.uniform(-5, 5),
            random.uniform(0.8, 1.0),
            random.uniform(0.8, 1.0),
            random.uniform(-0.05*(y1 - y0), 0.05*(y1 - y0)),
            random.uniform(-0.05*(x1 - x0), 0.05*(x1 - x0))
            ), trans)
    trans = np.dot(np.array([[1, 0, 0.5*(y1 + y0)], [0, 1, 0.5*(x1 + x0)], [0, 0, 1]]), trans)

    
    
    # Apply affine transformation
    matrix = trans[:2,:2]
    offset = trans[:2,2]
    img    = img.reshape(img.shape[:-1])
    img    = affine_transform(img, matrix, offset, output_shape=img_shape[:-1], order=1, mode='constant', cval=np.average(img))
    img    = img.reshape(img_shape2)

    # Normalize to zero mean and unit variance
    img  -= np.mean(img, keepdims=True)
    img  /= np.std(img, keepdims=True) + K.epsilon()
    img=array_to_img(img)
    
    img=img.convert('RGB')
    
    img=img_to_array(img)
    
    return img

def read_for_training(p):
    
    return read_cropped_image(p, True)

def read_for_validation(p):
    return read_cropped_image(p,False)

###  BRANCH MODEL  ###
branch_model=applications.xception.Xception(include_top=True,weights='imagenet')
for layer in branch_model.layers[:]:
    layer.trainable=False
branch_model.summary()

###   HEAD MODEL   ###
    
xa_inp = Input(shape=branch_model.output_shape[1:])
xb_inp = Input(shape=branch_model.output_shape[1:])
x1 = Lambda(lambda x: x[0] * x[1])([xa_inp, xb_inp])
x2 = Lambda(lambda x: x[0] + x[1])([xa_inp, xb_inp])
x3 = Lambda(lambda x: K.abs(x[0] - x[1]))([xa_inp, xb_inp])
x4 = Lambda(lambda x: K.square(x))(x3)
x5=  Lambda(lambda x: x[0]*K.log(x[1]))([xa_inp, xb_inp])
x6=  Lambda(lambda x: x[1]*K.log(x[0]))([xa_inp, xb_inp])
x = Concatenate()([x1, x2, x3, x4, x5 ,x6 ])
x = Reshape((6, branch_model.output_shape[1], 1), name='reshape1')(x)
 
    
    
x=Dense(6,input_dim=(),activation='relu')(x)
x=Dense(8,activation='relu')(x)
x=Dense(16,activation='relu')(x)
x=Dense(1,activation='softmax')(x)
#x= Lambda(lambda y: y)(x)

print(type(x))  

head_model = Model([xa_inp, xb_inp], x, name='head')

## Siamese Network ###
    
img_a=Input(shape=img_shape)
img_b=Input(shape=img_shape)
print(img_a.shape)
xa=branch_model(img_a)
xb=branch_model(img_b)
x=head_model([xa,xb])
model_final=Model([img_a,img_b],x)
model_final.compile(loss='mean_squared_error',optimizer=optimizers.Adam(0.001),metrics=['acc'])

features=[]
for i in tqdm(range(len(train))):
    temp=np.zeros((1,299,299,3))
    temp[0]=read_for_validation(train[i])/255
    p=branch_model.predict(temp)
    
    features.append(p)

pickle_out = open("features.pickle","wb")
pickle.dump(features, pickle_out)
pickle_out.close()

import gc
score=np.zeros((len(train),len(train)))
for j in tqdm(range(len(train))):
    for i in range(len(train)):
        score[j][i]=np.sum(np.abs(features[i]-features[j]))



score=np.round(score*1000)
score=-score
score

import pickle
pickle_out = open("dict.pickle","wb")
pickle.dump(score, pickle_out)
pickle_out.close()





!pip3 install lapjv

from lapjv import lapjv

pickle_in=open('dict.pickle','rb')
score=pickle.load(pickle_in)
pickle_in=open('pos.pickle','rb')
position=pickle.load(pickle_in)

np.min(score)

max_len=0
for v,k in h2ws.items():
    if len(k)>1:
        if len(k)>max_len:
            max_len=len(k)
        for i in k:
            for j in k:
                score[position[i]][position[j]]=0

reverse_position={}
for k,v in position.items():
    reverse_position[v]=k

score=2000+score
np.max(score)

np.min(score)

pairs={}
for i in tqdm(range(max_len)):
    
    row,col,a=lapjv(score)
    
    for i in range(len(row)):
        if reverse_position[row[i]] not in pairs.keys():
            pairs[reverse_position[row[i]]]=[reverse_position[col[i]]]
        else:
            pairs[reverse_position[row[i]]].append(reverse_position[col[i]])
        
    
        score[row[i]][col[i]]=2000



import pickle
pickle_out = open("pair.pickle","wb")
pickle.dump(pairs, pickle_out)
pickle_out.close()

pickle_out=open('pair.pickle','rb')
pairs=pickle.load(pickle_out)

training_data=[]
y=[]
for k,v in h2ws.items():
  temp_len=len(v)
  if temp_len>1:
    for i in v:
      for j in v:
        training_data.append((i,j))
        y.append(1)
      
    for i in v:
      for j in range(temp_len):
        training_data.append((i,pairs[i][j]))
        y.append(0)

X_train=[]
for i in traning_data:
  x,y=i
  x=read_for_validation(x)
  y=read_for_validation(y)
  
  X_train.append([x,y])

model_final.fit(X_train,y,epochs=32.verbose=1,batch_size=32)































